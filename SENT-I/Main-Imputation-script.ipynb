{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTCSV_PATH = '/root/workspace/FAISS-Imputation/data-imputation/Data/files-to-process/SHUFFLED'\n",
    "STARTTXT= 'iTuneAmazonS'\n",
    "STARTTXTFP=STARTCSV_PATH+STARTTXT+'.txt'\n",
    "STARTCSVFP=STARTTXTFP[:STARTTXTFP.rfind('.')]+'.csv'\n",
    "SENTENCETRANSFORMER='sentence-transformers/LaBSE'\n",
    "cumulative_percentages = [0.0, 0.1, 0.1, 0.1, 0.1]\n",
    "cumulative_percentages = [sum(cumulative_percentages[:i + 1]) for i in range(len(cumulative_percentages))]\n",
    "# seeds = random.sample(range(1, 10000), 10)\n",
    "seeds = [98, 409, 1753, 8424, 2782, 6004, 3581, 5553, 4721, 7179]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Song_Name', 'Artist_Name', 'Album_Name', 'Genre', 'Price', 'CopyRight', 'Time', 'Released', 'Song_Name', 'Artist_Name', 'Album_Name', 'Genre', 'Price', 'CopyRight', 'Time', 'Released', 'label']\n",
      "['Song_Name', 'Artist_Name', 'Album_Name', 'Genre', 'Price', 'CopyRight', 'Time', 'Released', 'Song_Name', 'Artist_Name', 'Album_Name', 'Genre', 'Price', 'CopyRight', 'Time', 'Released', 'label']\n",
      "['Song_Name', 'Artist_Name', 'Album_Name', 'Genre', 'Price', 'CopyRight', 'Time', 'Released', 'Song_Name', 'Artist_Name', 'Album_Name', 'Genre', 'Price', 'CopyRight', 'Time', 'Released', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Function to process individual file content\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        giantlist = []\n",
    "        for line in file:\n",
    "            col_val_pairs = re.findall(r'COL(.*?)VAL(.*?)(?=COL|$)', line)\n",
    "            if col_val_pairs:\n",
    "                listrowi = []\n",
    "                for col, val in col_val_pairs:\n",
    "                    col_value = col.strip()\n",
    "                    val_value = val.strip()\n",
    "                    listrowi.append([col_value, val_value])\n",
    "                last_val = listrowi[-1][1]\n",
    "                label = str(last_val)[-1]\n",
    "                listrowi[-1][1] = last_val[:-2].strip()\n",
    "                listrowi.append(['label', label])\n",
    "                giantlist.append(listrowi)\n",
    "            else:\n",
    "                print(f\"No 'COL' or 'VAL' pairs found in line: {line.strip()}\")\n",
    "    return giantlist\n",
    "\n",
    "# Function to save content into CSV\n",
    "def save_to_csv(r, csv_file_path):\n",
    "    headers = [str(x[0]) for x in r[0]]\n",
    "    print(headers)\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(headers)\n",
    "\n",
    "        for record in r:\n",
    "            row = [str(x[1]) for x in record]\n",
    "            csvwriter.writerow(row)\n",
    "\n",
    "# Function to join all related files and process them\n",
    "def join_and_process_files(folder_path):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    data_files = [file for file in all_files if file.rfind(STARTTXT) != -1 and file.endswith('.txt')]\n",
    "\n",
    "    for file_name in data_files:\n",
    "        STARTTXTFP = os.path.join(folder_path, file_name)\n",
    "        STARTCSVFP = STARTTXTFP[:STARTTXTFP.rfind('.')] + '.csv'\n",
    "        # print(f\"Processing File: {STARTTXTFP}\")\n",
    "        # print(f\"Saving as CSV: {STARTCSVFP}\")\n",
    "        records = process_file(STARTTXTFP)\n",
    "        save_to_csv(records, STARTCSVFP)\n",
    "\n",
    "# Example usage\n",
    "join_and_process_files(STARTCSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to split the CSV file into 3 subsets\n",
    "def split_csv(file_path):\n",
    "    df = pd.read_csv(file_path, header=None, encoding='utf-8')\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df.reindex(df.index.drop(0)).reset_index(drop=True)\n",
    "    headers = list(df.columns)\n",
    "    g = int((len(headers) - 1) / 2)\n",
    "    subset1 = df.iloc[:, :g]\n",
    "    subset2 = df.iloc[:, g:-1]\n",
    "    subset3 = df.iloc[:, -1]\n",
    "\n",
    "    # Construct paths for the subsets\n",
    "    subset1_path = file_path[:file_path.rfind('.')] + '_1.csv'\n",
    "    subset2_path = file_path[:file_path.rfind('.')] + '_2.csv'\n",
    "    subset3_path = file_path[:file_path.rfind('.')] + '_3.csv'\n",
    "    subset1.to_csv(subset1_path, index=False, header=True, encoding='utf-8')\n",
    "    subset2.to_csv(subset2_path, index=False, header=True, encoding='utf-8')\n",
    "    subset3 = subset3.astype(int)\n",
    "    subset3.to_csv(subset3_path, index=False, header=True, encoding='utf-8')\n",
    "    return subset1_path, subset2_path, subset3_path\n",
    "\n",
    "def rejoin_splits_and_save(STARTCSV_PATH):\n",
    "    all_files = os.listdir(STARTCSV_PATH)\n",
    "    prefixes = set([file[:file.rfind('_')] for file in all_files if file.endswith('.csv')])\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        subset1_path = os.path.join(STARTCSV_PATH, prefix + '_1.csv')\n",
    "        subset2_path = os.path.join(STARTCSV_PATH, prefix + '_2.csv')\n",
    "\n",
    "        if os.path.exists(subset1_path) and os.path.exists(subset2_path):\n",
    "            subset1_loaded = pd.read_csv(subset1_path, encoding='utf-8')\n",
    "            subset2_loaded = pd.read_csv(subset2_path, encoding='utf-8')\n",
    "\n",
    "            result = pd.concat([subset1_loaded, subset2_loaded], axis=0, ignore_index=True)\n",
    "            result_path = os.path.join(STARTCSV_PATH, prefix + '_12.csv')\n",
    "            result.to_csv(result_path, index=False, header=True, encoding='utf-8')\n",
    "\n",
    "            print(f\"Rejoined subsets saved to {result_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping {prefix} as one or both subset files are missing\")\n",
    "all_csv_files = [file for file in os.listdir(STARTCSV_PATH) if file.endswith('.csv') and '_12.csv' not in file]\n",
    "\n",
    "for file in all_csv_files:\n",
    "    file_path = os.path.join(STARTCSV_PATH, file)\n",
    "    split_csv(file_path)\n",
    "rejoin_splits_and_save(STARTCSV_PATH)\n",
    "dataframes = []\n",
    "for file_name in os.listdir(STARTCSV_PATH):\n",
    "    if file_name.endswith('_12.csv') and file_name.startswith('test'):\n",
    "        file_path = os.path.join(STARTCSV_PATH, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            df['source_file'] = file_name\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"{file_name} does not exist.\")\n",
    "result = pd.concat(dataframes, axis=0, ignore_index=False)\n",
    "result_path = os.path.join(STARTCSV_PATH, 'combined_all_files.csv')\n",
    "result.to_csv(result_path, index=False, header=True, encoding='utf-8')\n",
    "\n",
    "print(f\"Combined CSV saved to {result_path}\")\n",
    "specific_files_to_remove = [\n",
    "    os.path.join(STARTCSV_PATH, f'test_{STARTTXT}.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'test_{STARTTXT}_1.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'test_{STARTTXT}_2.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'test_{STARTTXT}_12.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'train_{STARTTXT}.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'train_{STARTTXT}_1.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'train_{STARTTXT}_2.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'train_{STARTTXT}_12.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'valid_{STARTTXT}.csv'),    \n",
    "    os.path.join(STARTCSV_PATH, f'valid_{STARTTXT}_1.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'valid_{STARTTXT}_2.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'valid_{STARTTXT}_12.csv'),\n",
    "]\n",
    "for file_name in specific_files_to_remove:\n",
    "    print(file_name)\n",
    "    if os.path.isfile(file_name):\n",
    "        os.remove(file_name)  \n",
    "\n",
    "print(\"Cleanup complete. Only the specified files are kept.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inject null values without nulling entire rows\n",
    "def inject_nulls(dataframe, target_percentage, current_percentage, seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    additional_percentage = target_percentage - current_percentage\n",
    "    print(f\"Additional percentage needed: {additional_percentage}\")\n",
    "\n",
    "    num_rows, num_cols = dataframe.shape\n",
    "    num_cols_to_inject = num_cols - 1\n",
    "    null_counts = [int((num_rows - 1) * additional_percentage) for _ in range(num_cols_to_inject)]\n",
    "\n",
    "    # Create a copy of the DataFrame to inject nulls\n",
    "    modified_df = dataframe.copy()\n",
    "    for col_idx in range(num_cols_to_inject):\n",
    "        col_null_count = null_counts[col_idx]\n",
    "        if col_null_count > 0:\n",
    "            available_indices = modified_df.index[modified_df.iloc[:, col_idx].notna()].tolist()\n",
    "            if len(available_indices) < col_null_count:\n",
    "                col_null_count = len(available_indices)\n",
    "            if col_null_count > 0 and available_indices:\n",
    "                null_indices = random.sample(available_indices, col_null_count)\n",
    "                modified_df.iloc[null_indices, col_idx] = np.nan\n",
    "\n",
    "    return modified_df\n",
    "\n",
    "\n",
    "# Calculate and print percentage of missing values in each column\n",
    "def print_missing_value_percentage(dataframe):\n",
    "    missing_percentage = dataframe.isnull().mean() * 100\n",
    "    print(\"Percentage of missing values in each column:\")\n",
    "    print(missing_percentage)\n",
    "    \n",
    "df = pd.read_csv(result_path, encoding='utf-8')\n",
    "for seed in seeds:\n",
    "    print(f\"Processing seed: {seed}\")\n",
    "    current_df = df.copy()\n",
    "    current_percentage = 0\n",
    "    \n",
    "    for target_percentage in cumulative_percentages:\n",
    "        print(f\"Processing percentage: {target_percentage} for seed {seed}\")\n",
    "        modified_df = inject_nulls(current_df, target_percentage, current_percentage, seed)\n",
    "        current_df = modified_df\n",
    "        current_percentage = target_percentage\n",
    "\n",
    "        print(f\"\\nMissing values for {int(target_percentage * 100)}% nulls (seed {seed}):\")\n",
    "        print_missing_value_percentage(modified_df)\n",
    "\n",
    "        output_file_name = os.path.join(STARTCSV_PATH, f'{STARTTXT}_{(current_percentage)}_{seed}_nonimputed.csv')\n",
    "        modified_df.to_csv(output_file_name, index=False, header=True, encoding='utf-8')\n",
    "        print(f\"Saved: {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Process is Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Imputation using FAISS\n",
    "\n",
    "def get_map_dq_d1n(name):\n",
    "    if \"DBLPACM\" in name:\n",
    "        return [0,1,2,3]\n",
    "    if \"walmartAmazon\" in name:\n",
    "        return [0,1,2,3,4]\n",
    "    if \"DBLPScholar\" in name:\n",
    "        return [0,1,2,3]\n",
    "\n",
    "def impute_values(dq, dfaiss, map_dq_d1n, similarity_threshold):\n",
    "    results = []\n",
    "    for row in dq.values:\n",
    "        query = ' '.join([str(i) if not pd.isna(i) else \" \" for i in row[:-1]])\n",
    "        docs_and_scores = dfaiss.similarity_search_with_relevance_scores(query)\n",
    "        \n",
    "        row_copy = row.copy().tolist()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            if pd.isna(row_copy[i]):\n",
    "                missing_position = i\n",
    "                for emb, score in docs_and_scores:\n",
    "                    if score >= similarity_threshold:\n",
    "                        candidate = emb.metadata[feature_names_d1n[map_dq_d1n[missing_position]]]\n",
    "                        row_copy[missing_position] = candidate\n",
    "                        if not pd.isna(row_copy[missing_position]):\n",
    "                            break\n",
    "        results.append(row_copy)\n",
    "\n",
    "    results = pd.DataFrame(results, columns=feature_names_d1n)\n",
    "    return results\n",
    "\n",
    "# MAIN IMPUTATION LOOP\n",
    "for seed in seeds:\n",
    "    for null_percentage in [\"0.0\", \"0.1\", \"0.2\", \"0.3\", \"0.4\"]:\n",
    "        initail_dataset = STARTCSV_PATH + \"/\" + STARTTXT + \"_\" + str(null_percentage) + \"_\" + str(seed) + \"_nonimputed.csv\"\n",
    "        cs = pd.read_csv(initail_dataset, encoding='utf-8')\n",
    "        d1n = cs\n",
    "        feature_names_d1n = list(d1n.columns.values)\n",
    "        map_dq_d1n = get_map_dq_d1n(STARTTXT)\n",
    "        docs = []\n",
    "\n",
    "        for row in d1n.values:\n",
    "            key = ' '.join([str(i) if not pd.isna(i) else \" \" for i in row[:-1]])\n",
    "            docs.append(Document(page_content=key, metadata={k: v for k, v in zip(feature_names_d1n, row)}))\n",
    "\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=SENTENCETRANSFORMER)\n",
    "        dfaiss = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "        initail_dataset_imputed_fp = STARTCSV_PATH + \"/\" + STARTTXT + \"_\" + str(null_percentage) + \"_\" + str(seed) + \"_imputed.csv\"\n",
    "        dq2 = cs\n",
    "        dq2.isnull().mean() * 100\n",
    "        feature_names_dq2 = list(dq2.columns.values)\n",
    "        results2 = impute_values(dq2, dfaiss, map_dq_d1n, 0.50)\n",
    "        results2.to_csv(initail_dataset_imputed_fp, index=False)\n",
    "\n",
    "        del dfaiss\n",
    "        gc.collect()\n",
    "        print(\"FAISS file removed for \" + initail_dataset)\n",
    "\n",
    "\n",
    "# Clear the output of the executed cell\n",
    "clear_output(wait=True)\n",
    "print(\"The Process is Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Process is Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Function to split dataframe based on the last column (source_file)\n",
    "def split_based_on_last_column(dataframe, percentage, seed, imputed_status, STARTCSV_PATH, original_file):\n",
    "    unique_labels = dataframe.iloc[:, -1].unique()\n",
    "    print(f\"Unique labels found: {unique_labels}\")\n",
    "\n",
    "    label_mapping = {\n",
    "        'train': 'training_file',\n",
    "        'test': 'testing_file',\n",
    "        'valid': 'validation_file'\n",
    "    }\n",
    "\n",
    "    files_created = []\n",
    "    for label in unique_labels:\n",
    "        label_core = label.replace('.csv', '').split('_')[0].lower()\n",
    "        new_label = label_mapping.get(label_core, None)\n",
    "\n",
    "        if new_label is None:\n",
    "            print(f\"Skipping unknown label: {label}\")\n",
    "            continue\n",
    "        subset_df = dataframe[dataframe.iloc[:, -1] == label]\n",
    "        print(f\"Processing label: {new_label}, Rows: {len(subset_df)}\")\n",
    "\n",
    "        # Construct output file path\n",
    "        output_file_name = os.path.join(STARTCSV_PATH, f'{new_label}_{percentage}_{seed}_{imputed_status}.csv')\n",
    "\n",
    "        # Save the subset to a new CSV file\n",
    "        if not subset_df.empty:\n",
    "            subset_df.to_csv(output_file_name, index=False, header=True, encoding='utf-8')\n",
    "            print(f\"Saved: {output_file_name}\")\n",
    "            remove_last_column(output_file_name)\n",
    "            files_created.append(output_file_name)\n",
    "        else:\n",
    "            print(f\"No data to save for {new_label}.\")\n",
    "def remove_last_column(file_path):\n",
    "    try:\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "        df = df.iloc[:, :-1]\n",
    "        df.to_csv(file_path, index=False, header=True, encoding='utf-8')\n",
    "        print(f\"Removed last column and saved: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "# Process each file in the folder\n",
    "for file_name in os.listdir(STARTCSV_PATH):\n",
    "    if file_name.startswith(f'{STARTTXT}') and (file_name.endswith('_imputed.csv') or file_name.endswith('_nonimputed.csv')):\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        parts = file_name.replace('.csv', '').split('_')\n",
    "        if len(parts) == 4:\n",
    "            percentage = parts[1]\n",
    "            seed = parts[2]\n",
    "            imputed_status = parts[3]\n",
    "\n",
    "            file_path = os.path.join(STARTCSV_PATH, file_name)\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding='utf-8')\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if df.empty:\n",
    "                print(f\"DataFrame is empty for file: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            # Split the dataframe based on the last column and delete source file\n",
    "            split_based_on_last_column(df, percentage, seed, imputed_status, STARTCSV_PATH, file_path)\n",
    "        else:\n",
    "            print(f\"File name format not matching expected pattern: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Skipping file: {file_name} (doesn't match)\")\n",
    "\n",
    "file_patterns = [\"testing_file_*_*_imputed.csv\", \"training_file_*_*_imputed.csv\", \"validation_file_*_*_imputed.csv\", \n",
    "                 \"testing_file_*_*_nonimputed.csv\", \"training_file_*_*_nonimputed.csv\", \"validation_file_*_*_nonimputed.csv\"]\n",
    "additional_files = {\n",
    "    \"testing_file\": f'test_{STARTTXT}_3.csv',\n",
    "    \"training_file\": f'train_{STARTTXT}_3.csv',\n",
    "    \"validation_file\": f'valid_{STARTTXT}_3.csv'\n",
    "}\n",
    "\n",
    "def split_file_horizontally(file_path, save_dir):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if len(df) < 2:\n",
    "        print(f\"Error: {file_path} does not have enough rows to split.\")\n",
    "        return None, None\n",
    "    half_index = len(df) // 2\n",
    "    df_first_half = df.iloc[:half_index, :]\n",
    "    df_second_half = df.iloc[half_index:, :]\n",
    "    base_name = os.path.basename(file_path).replace('.csv', '')\n",
    "    first_half_file = os.path.join(save_dir, f\"{base_name}_first_half.csv\")\n",
    "    second_half_file = os.path.join(save_dir, f\"{base_name}_second_half.csv\")\n",
    "\n",
    "    df_first_half.to_csv(first_half_file, index=False)\n",
    "    df_second_half.to_csv(second_half_file, index=False)\n",
    "\n",
    "    print(f\"Split {file_path} into:\")\n",
    "    print(f\"  - {first_half_file} with shape {df_first_half.shape}\")\n",
    "    print(f\"  - {second_half_file} with shape {df_second_half.shape}\")\n",
    "\n",
    "    return first_half_file, second_half_file\n",
    "def merge_files_horizontally(file1_path, file2_path, save_dir):\n",
    "    df_first_half = pd.read_csv(file1_path)\n",
    "    df_second_half = pd.read_csv(file2_path)\n",
    "    df_merged = pd.concat([df_first_half, df_second_half], axis=1)\n",
    "\n",
    "    # Save the merged DataFrame\n",
    "    base_name = os.path.basename(file1_path).replace('_first_half.csv', '')\n",
    "    merged_file = os.path.join(save_dir, f\"{base_name}_merged.csv\")\n",
    "    df_merged.to_csv(merged_file, index=False)\n",
    "\n",
    "    print(f\"Merged {file1_path} and {file2_path} into:\")\n",
    "    print(f\"  - {merged_file} with shape {df_merged.shape}\")\n",
    "\n",
    "    return merged_file\n",
    "\n",
    "# Function to merge the additional file based on naming pattern\n",
    "def merge_additional_file(merged_file_path, additional_file_name, save_dir):\n",
    "    df_merged = pd.read_csv(merged_file_path)\n",
    "    df_additional = pd.read_csv(os.path.join(save_dir, additional_file_name))\n",
    "    df_final = pd.concat([df_merged, df_additional], axis=1)\n",
    "    final_file_name = merged_file_path.replace('_merged.csv', '_final.csv')\n",
    "    df_final.to_csv(final_file_name, index=False)\n",
    "\n",
    "    print(f\"Merged {merged_file_path} with {additional_file_name} into:\")\n",
    "    print(f\"  - {final_file_name} with shape {df_final.shape}\")\n",
    "\n",
    "    return merged_file_path, final_file_name\n",
    "files_to_delete = []\n",
    "\n",
    "# Loop through each pattern and process matching files\n",
    "for pattern in file_patterns:\n",
    "    files = glob.glob(os.path.join(STARTCSV_PATH, pattern))\n",
    "\n",
    "    for file_path in files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        first_half_file, second_half_file = split_file_horizontally(file_path, STARTCSV_PATH)\n",
    "        if first_half_file and second_half_file:\n",
    "            merged_file = merge_files_horizontally(first_half_file, second_half_file, STARTCSV_PATH)\n",
    "\n",
    "            # Determine the additional file to merge based on the naming pattern\n",
    "            for key, additional_file in additional_files.items():\n",
    "                if key in pattern:\n",
    "                    final_file_name = merge_additional_file(merged_file, additional_file, STARTCSV_PATH)\n",
    "                    files_to_delete.extend([first_half_file, second_half_file, merged_file])\n",
    "\n",
    "for file in files_to_delete:\n",
    "    os.remove(file)\n",
    "    print(f\"Deleted file: {file}\")\n",
    "files_to_remove_patterns = [\n",
    "    os.path.join(STARTCSV_PATH, 'testing_file_*_*_imputed.csv'),\n",
    "    os.path.join(STARTCSV_PATH, 'testing_file_*_*_nonimputed.csv'),\n",
    "    os.path.join(STARTCSV_PATH, 'training_file_*_*_imputed.csv'),\n",
    "    os.path.join(STARTCSV_PATH, 'training_file_*_*_nonimputed.csv'),\n",
    "    os.path.join(STARTCSV_PATH, 'validation_file_*_*_imputed.csv'),\n",
    "    os.path.join(STARTCSV_PATH, 'validation_file_*_*_nonimputed.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'test_{STARTTXT}_3.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'train_{STARTTXT}_3.csv'),\n",
    "    os.path.join(STARTCSV_PATH, f'valid_{STARTTXT}_3.csv'),\n",
    "    \n",
    "]\n",
    "\n",
    "for pattern in files_to_remove_patterns:\n",
    "    files_to_remove = glob.glob(pattern)\n",
    "    for file_path in files_to_remove:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted file: {file_path}\")\n",
    "        else:\n",
    "            print(f\"File not found or not a file: {file_path}\")\n",
    "\n",
    "# Function to convert a CSV file to a TXT file\n",
    "def write_to_text(csv_file_path):\n",
    "    text_file_path = csv_file_path[:csv_file_path.rfind('.')] + \".txt\"\n",
    "    data = []\n",
    "\n",
    "    with open(csv_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        headers = next(csvreader)\n",
    "\n",
    "        for row in csvreader:\n",
    "            record = []\n",
    "            for i in range(len(headers)):\n",
    "                if row[i].strip():\n",
    "                    record.append([headers[i].strip(), row[i].strip()])\n",
    "                else:\n",
    "                    record.append([headers[i].strip(), ''])\n",
    "            data.append(record)\n",
    "\n",
    "        with open(text_file_path, 'w', encoding='utf-8') as file:\n",
    "            names = headers\n",
    "            for record in data:\n",
    "                next_index = 0\n",
    "                last_value_written = \"\"\n",
    "\n",
    "                for i, pair in enumerate(record):\n",
    "                    col, val = pair\n",
    "                    while next_index < len(names) and col != names[next_index]:\n",
    "                        if next_index == ((len(names) - 1) // 2 - 1):\n",
    "                            file.write(f\"COL {names[next_index]} VAL  \\t\")\n",
    "                        elif names[next_index] == 'label':\n",
    "                            pass\n",
    "                        else:\n",
    "                            file.write(f\"COL {names[next_index]} VAL \")\n",
    "                        next_index += 1\n",
    "                    if next_index < len(names):\n",
    "                        if next_index == ((len(names) - 1) // 2 - 1):\n",
    "                            file.write(f\"COL {col} VAL {val} \\t\")\n",
    "                        elif col == 'label':\n",
    "                            last_value_written = val\n",
    "                        else:\n",
    "                            file.write(f\"COL {col} VAL {val} \")\n",
    "                        next_index += 1\n",
    "                while next_index < len(names):\n",
    "                    if next_index == ((len(names) - 1) // 2 - 1):\n",
    "                        file.write(f\"COL {names[next_index]} VAL \\t\")\n",
    "                    elif names[next_index] == 'label':\n",
    "                        pass\n",
    "                    else:\n",
    "                        file.write(f\"COL {names[next_index]} VAL \")\n",
    "                    next_index += 1\n",
    "                file.write(f\"\\t{last_value_written}\\n\")\n",
    "\n",
    "\n",
    "# Get all final CSV files in the directory (change the pattern if needed)\n",
    "final_csv_files = glob.glob(os.path.join(STARTCSV_PATH, \"*_final.csv\"))\n",
    "\n",
    "for csv_file in final_csv_files:\n",
    "    print(f\"Converting {csv_file} to TXT...\")\n",
    "    write_to_text(csv_file)\n",
    "\n",
    "print(\"Conversion completed.\")\n",
    "file_patterns = [\n",
    "    \"testing_file_*_*_imputed_final.csv\",\n",
    "    \"training_file_*_*_imputed_final.csv\",\n",
    "    \"validation_file_*_*_imputed_final.csv\",\n",
    "    \"testing_file_*_*_nonimputed_final.csv\",\n",
    "    \"training_file_*_*_nonimputed_final.csv\",\n",
    "    \"validation_file_*_*_nonimputed_final.csv\"\n",
    "]\n",
    "def delete_files_by_pattern(base_path, file_patterns):\n",
    "    for pattern in file_patterns:\n",
    "        files = glob.glob(os.path.join(base_path, pattern))\n",
    "        print(f\"Found files matching pattern '{pattern}': {files}\")\n",
    "        for file_path in files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted file: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting file {file_path}: {e}\")\n",
    "delete_files_by_pattern(STARTCSV_PATH, file_patterns)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Clear the output of the executed cell\n",
    "clear_output(wait=True)\n",
    "print(\"The Process is Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Process is Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "folder_structure = {\n",
    "    f'imputed_{STARTTXT}': '*_imputed_final.txt',\n",
    "    f'non-imputed_{STARTTXT}': '*_nonimputed_final.txt',\n",
    "    f'imputed_CSV_{STARTTXT}': '*.csv'\n",
    "}\n",
    "\n",
    "TARGET_PATH_1 = '/root/workspace/FAISS-Imputation/data-imputation/Data/processed_files/iTunes-Amazon/THRESHOLD50'\n",
    "TARGET_PATH_2 = '/root/workspace/FAISS-Imputation/ditto-master/data/SHUFFLED/iTunes-Amazon/THRESHOLD50'\n",
    "def create_main_folders(target_paths):\n",
    "    for target_path in target_paths:\n",
    "        os.makedirs(target_path, exist_ok=True)\n",
    "        print(f\"Created main folder: {target_path}\")\n",
    "def create_folders(target_paths, folder_structure):\n",
    "    for target_path in target_paths:\n",
    "        for subfolder in folder_structure:\n",
    "            subfolder_path = os.path.join(target_path, subfolder)\n",
    "            os.makedirs(subfolder_path, exist_ok=True)\n",
    "            print(f\"Created folder: {subfolder_path}\")\n",
    "def copy_and_remove_files(target_paths, folder_structure):\n",
    "    for subfolder, pattern in folder_structure.items():\n",
    "        files = glob.glob(os.path.join(STARTCSV_PATH, pattern))\n",
    "        print(f\"Looking for files with pattern: {pattern}\")\n",
    "        print(f\"Found files: {files}\")\n",
    "        for file_path in files:\n",
    "            for target_path in target_paths:\n",
    "                target_folder = os.path.join(target_path, subfolder)\n",
    "                shutil.copy(file_path, target_folder)\n",
    "                print(f\"Copied {file_path} to {target_folder}\")\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed {file_path} from source directory\")\n",
    "def delete_folder_from_target(target_path, folder_name):\n",
    "    folder_path = os.path.join(target_path, folder_name)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Deleted folder: {folder_path}\")\n",
    "    else:\n",
    "        print(f\"Folder {folder_path} does not exist.\")\n",
    "target_paths = [TARGET_PATH_1, TARGET_PATH_2]\n",
    "create_main_folders(target_paths)\n",
    "\n",
    "# Create the subfolders in both target directories\n",
    "create_folders(target_paths, folder_structure)\n",
    "copy_and_remove_files(target_paths, folder_structure)\n",
    "delete_folder_from_target(TARGET_PATH_2, f'imputed_CSV_{STARTTXT}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tariq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
